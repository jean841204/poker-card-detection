import streamlit as st
from PIL import Image
import os
import sys
from pathlib import Path
import tempfile
import numpy as np
import cv2
import torch

# æ·»åŠ  YOLOv7 è·¯å¾‘åˆ°ç³»çµ±è·¯å¾‘
# å„ªå…ˆä½¿ç”¨å°ˆæ¡ˆå…§çš„ yolov7ï¼Œå…¶æ¬¡ä½¿ç”¨æœ¬åœ°è·¯å¾‘
YOLOV7_PATH = Path(__file__).parent / "yolov7"
if not YOLOV7_PATH.exists():
    YOLOV7_PATH = Path("/Users/jessica/Desktop/NCHU/ç ”ç©¶æ–¹æ³•è«–/yolov7")

if YOLOV7_PATH.exists():
    sys.path.insert(0, str(YOLOV7_PATH))

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="æ’²å…‹ç‰ŒèŠ±è‰²è¾¨è­˜å™¨",
    page_icon="ğŸƒ",
    layout="wide"
)

# æ¨™é¡Œ
st.title("ğŸƒ æ’²å…‹ç‰ŒèŠ±è‰²è¾¨è­˜ç³»çµ±")
st.markdown("---")

# å´é‚Šæ¬„ - æ¨¡å‹è³‡è¨Š
with st.sidebar:
    st.header("ğŸ“‹ ç³»çµ±è³‡è¨Š")
    st.info("ä½¿ç”¨ YOLOv7 é€²è¡Œæ’²å…‹ç‰ŒèŠ±è‰²åˆ†é¡")

    # æ¨¡å‹è¨­å®š
    st.header("âš™ï¸ æ¨è«–è¨­å®š")
    confidence = st.slider("ä¿¡å¿ƒåº¦é–¾å€¼", 0.0, 1.0, 0.25, 0.05)
    img_size = st.number_input("åœ–ç‰‡å¤§å°", min_value=320, max_value=1280, value=640, step=32)

# è¼‰å…¥æ¨¡å‹
@st.cache_resource
def load_model():
    model_path = Path(__file__).parent / "weight" / "best.pt"

    # æ–¹æ³• 1: å˜—è©¦ä½¿ç”¨æœ¬åœ° YOLOv7ï¼ˆæœ€å¯é ï¼‰
    if YOLOV7_PATH.exists():
        try:
            # å°å…¥æœ¬åœ° YOLOv7 çš„ models
            from models.experimental import attempt_load
            from utils.torch_utils import select_device

            device = select_device('cpu')  # ä½¿ç”¨ CPU
            model = attempt_load(str(model_path), map_location=device)
            model.conf = 0.25
            model.iou = 0.45

            # åŒ…è£æ¨¡å‹ä»¥æ”¯æ´ YOLOv7 çš„æ¨è«–ä»‹é¢
            class YOLOv7Wrapper:
                def __init__(self, model, device):
                    self.model = model
                    self.device = device
                    self.conf = 0.25
                    self.iou = 0.45
                    self.names = model.names if hasattr(model, 'names') else model.module.names

                def __call__(self, img, size=640):
                    from utils.general import non_max_suppression, scale_coords
                    from utils.datasets import letterbox
                    import torch

                    # é è™•ç†åœ–ç‰‡
                    img0 = img.copy()
                    img = letterbox(img, size, stride=32)[0]
                    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3xHxW
                    img = np.ascontiguousarray(img)
                    img = torch.from_numpy(img).to(self.device)
                    img = img.float()
                    img /= 255.0
                    if img.ndimension() == 3:
                        img = img.unsqueeze(0)

                    # æ¨è«–
                    with torch.no_grad():
                        pred = self.model(img)[0]

                    # NMS
                    pred = non_max_suppression(pred, self.conf, self.iou)

                    # è™•ç†æª¢æ¸¬çµæœ
                    class Results:
                        def __init__(self, pred, img0, img, names):
                            self.pred = pred
                            self.img0 = img0
                            self.img = img
                            self.names = names

                        def pandas(self):
                            import pandas as pd
                            class XYXYContainer:
                                def __init__(self, pred, names):
                                    self.data = []
                                    if pred[0] is not None and len(pred[0]):
                                        for *xyxy, conf, cls in pred[0].cpu().numpy():
                                            self.data.append({
                                                'xmin': xyxy[0],
                                                'ymin': xyxy[1],
                                                'xmax': xyxy[2],
                                                'ymax': xyxy[3],
                                                'confidence': conf,
                                                'class': int(cls),
                                                'name': names[int(cls)]
                                            })
                                    self.xyxy = [pd.DataFrame(self.data)]

                                def __getitem__(self, idx):
                                    return self.xyxy[idx]

                                def __len__(self):
                                    return len(self.xyxy)

                            return XYXYContainer(self.pred, self.names)

                        def render(self):
                            from utils.plots import plot_one_box
                            img = self.img0.copy()
                            if self.pred[0] is not None and len(self.pred[0]):
                                for *xyxy, conf, cls in self.pred[0].cpu().numpy():
                                    label = f'{self.names[int(cls)]} {conf:.2f}'
                                    plot_one_box(xyxy, img, label=label, line_thickness=2)
                            return [img]

                    # èª¿æ•´æª¢æ¸¬æ¡†åˆ°åŸåœ–å¤§å°
                    for det in pred:
                        if det is not None and len(det):
                            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()

                    return Results(pred, img0, img, self.names)

            wrapped_model = YOLOv7Wrapper(model, device)
            st.sidebar.success("âœ… ä½¿ç”¨æœ¬åœ° YOLOv7 è¼‰å…¥æ¨¡å‹")
            return wrapped_model

        except Exception as e:
            st.sidebar.warning(f"æœ¬åœ°è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ torch.hub: {e}")

    # æ–¹æ³• 2: ä½¿ç”¨ torch.hubï¼ˆå‚™ç”¨ï¼‰
    try:
        # è¨­å®šå®‰å…¨çš„å…¨åŸŸè®Šæ•¸ï¼ˆPyTorch 2.6+ éœ€è¦ï¼‰
        import numpy
        try:
            torch.serialization.add_safe_globals([numpy.core.multiarray._reconstruct])
        except:
            pass

        model = torch.hub.load('WongKinYiu/yolov7', 'custom',
                               path_or_model=str(model_path),
                               force_reload=False,
                               trust_repo=True,
                               _verbose=False)
        model.conf = 0.25
        model.iou = 0.45
        st.sidebar.success("âœ… ä½¿ç”¨ torch.hub è¼‰å…¥æ¨¡å‹")
        return model
    except Exception as e:
        st.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None

model = load_model()

if model is None:
    st.error("âš ï¸ ç„¡æ³•è¼‰å…¥æ¨¡å‹ï¼Œè«‹ç¢ºèª weight/best.pt æª”æ¡ˆå­˜åœ¨")
    st.stop()

# é¡¯ç¤ºæ¨¡å‹é¡åˆ¥
try:
    with st.sidebar:
        st.header("ğŸ·ï¸ å¯è¾¨è­˜é¡åˆ¥")
        if hasattr(model, 'names'):
            st.write(model.names)
        elif hasattr(model, 'module') and hasattr(model.module, 'names'):
            st.write(model.module.names)
except:
    pass

# æ¨è«–å‡½æ•¸
def predict_image(image, model, conf, imgsz):
    """å°åœ–ç‰‡é€²è¡Œæ¨è«–"""
    try:
        # è¨­å®šæ¨¡å‹åƒæ•¸
        model.conf = conf

        # å°‡ PIL Image è½‰æ›ç‚º numpy array
        img_np = np.array(image)

        # å¦‚æœæ˜¯ RGBAï¼Œè½‰æ›ç‚º RGB
        if img_np.shape[-1] == 4:
            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2RGB)

        # é€²è¡Œæ¨è«–
        results = model(img_np, size=imgsz)

        return results
    except Exception as e:
        st.error(f"æ¨è«–éŒ¯èª¤: {e}")
        return None

# å½±ç‰‡æ¨è«–å‡½æ•¸ - ç”Ÿæˆè™•ç†å¾Œçš„å½±ç‰‡
def predict_video(video_path, model, conf, imgsz, process_every_frame=True):
    """å°å½±ç‰‡é€²è¡Œæ¨è«–ä¸¦ç”Ÿæˆå¸¶æœ‰æª¢æ¸¬æ¡†çš„æ–°å½±ç‰‡"""
    try:
        cap = cv2.VideoCapture(video_path)

        # å–å¾—å½±ç‰‡è³‡è¨Š
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # å»ºç«‹è¼¸å‡ºå½±ç‰‡
        # ä½¿ç”¨ H.264 ç·¨ç¢¼ä»¥ç¢ºä¿ç€è¦½å™¨å¯ä»¥æ’­æ”¾
        output_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name

        # å˜—è©¦ä½¿ç”¨ H.264 ç·¨ç¢¼ï¼ˆæœ€å»£æ³›æ”¯æ´ï¼‰
        # å¦‚æœå¤±æ•—å‰‡é€€å›åˆ° mp4v
        fourcc_options = [
            cv2.VideoWriter_fourcc(*'avc1'),  # H.264
            cv2.VideoWriter_fourcc(*'H264'),  # H.264 alternative
            cv2.VideoWriter_fourcc(*'X264'),  # H.264 alternative
            cv2.VideoWriter_fourcc(*'mp4v'),  # MPEG-4 fallback
        ]

        out = None
        for fourcc in fourcc_options:
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            if out.isOpened():
                break

        if not out or not out.isOpened():
            st.error("ç„¡æ³•å»ºç«‹å½±ç‰‡ç·¨ç¢¼å™¨")
            return None, None, None, None

        frame_count = 0
        all_detections = []

        progress_bar = st.progress(0)
        status_text = st.empty()

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # å°‡ OpenCV BGR æ ¼å¼è½‰æ›ç‚º RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # é€²è¡Œæ¨è«–
            model.conf = conf
            results = model(frame_rgb, size=imgsz)

            # æ”¶é›†æª¢æ¸¬çµæœç”¨æ–¼çµ±è¨ˆ
            try:
                detections = results.pandas().xyxy[0]
                for _, row in detections.iterrows():
                    all_detections.append(row['name'])
            except:
                pass

            # å–å¾—å¸¶æœ‰æª¢æ¸¬æ¡†çš„åœ–ç‰‡
            rendered_frame = np.squeeze(results.render())

            # è½‰å› BGR ç”¨æ–¼ OpenCV å¯«å…¥
            frame_bgr = cv2.cvtColor(rendered_frame, cv2.COLOR_RGB2BGR)
            out.write(frame_bgr)

            # æ›´æ–°é€²åº¦
            frame_count += 1
            progress = min(frame_count / total_frames, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"è™•ç†é€²åº¦: {frame_count}/{total_frames} å¹€")

        cap.release()
        out.release()
        progress_bar.empty()
        status_text.empty()

        return output_path, all_detections, fps, total_frames
    except Exception as e:
        st.error(f"å½±ç‰‡è™•ç†éŒ¯èª¤: {e}")
        return None, None, None, None

# é¡¯ç¤ºæ¨è«–çµæœ
def display_results(results, image):
    """é¡¯ç¤ºæ¨è«–çµæœï¼ˆYOLOv7 æª¢æ¸¬æ ¼å¼ï¼‰"""
    if results is None:
        st.warning("æ²’æœ‰æª¢æ¸¬åˆ°ä»»ä½•çµæœ")
        return

    try:
        # ç²å–æª¢æ¸¬çµæœ
        detections = results.pandas().xyxy[0]
    except Exception as e:
        st.error(f"ç„¡æ³•è§£ææª¢æ¸¬çµæœ: {e}")
        return

    # é¡¯ç¤ºåœ–ç‰‡
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ“· åŸå§‹åœ–ç‰‡")
        st.image(image, use_container_width=True)

    with col2:
        st.subheader("ğŸ” æª¢æ¸¬çµæœåœ–")
        # é¡¯ç¤ºå¸¶æœ‰æª¢æ¸¬æ¡†çš„åœ–ç‰‡
        rendered_img = np.squeeze(results.render())
        st.image(rendered_img, use_container_width=True)

    # é¡¯ç¤ºæª¢æ¸¬è©³æƒ…
    st.subheader("ğŸ“Š æª¢æ¸¬è©³æƒ…")

    if len(detections) == 0:
        st.info("æœªæª¢æ¸¬åˆ°ä»»ä½•ç‰©é«”")
    else:
        # çµ±è¨ˆå„é¡åˆ¥æ•¸é‡
        class_counts = detections['name'].value_counts()

        col1, col2 = st.columns(2)

        with col1:
            st.write("**æª¢æ¸¬åˆ°çš„ç‰©é«”:**")
            for idx, row in detections.iterrows():
                st.metric(
                    label=f"ç‰©é«” {idx + 1}: {row['name']}",
                    value=f"{row['confidence']*100:.2f}%"
                )

        with col2:
            st.write("**é¡åˆ¥çµ±è¨ˆ:**")
            for class_name, count in class_counts.items():
                st.write(f"- **{class_name}**: {count} å€‹")

        # é¡¯ç¤ºæœ€é«˜ä¿¡å¿ƒåº¦çš„æª¢æ¸¬
        top_detection = detections.loc[detections['confidence'].idxmax()]
        st.success(f"### ğŸ¯ æœ€é«˜ä¿¡å¿ƒåº¦: **{top_detection['name']}** ({top_detection['confidence']*100:.2f}%)")

# ä¸»è¦å…§å®¹å€åŸŸ - ä½¿ç”¨ radio é¸æ“‡æ¨¡å¼
if 'page_mode' not in st.session_state:
    st.session_state['page_mode'] = "ğŸ“¤ ä¸Šå‚³åœ–ç‰‡"

page_mode = st.radio(
    "é¸æ“‡è¼¸å…¥æ–¹å¼",
    ["ğŸ“¤ ä¸Šå‚³åœ–ç‰‡", "ğŸ¬ ä¸Šå‚³å½±ç‰‡", "ğŸ“ é¸æ“‡ç¯„ä¾‹åœ–ç‰‡"],
    horizontal=True,
    key='page_mode'
)

# ç•¶åˆ‡æ›æ¨¡å¼æ™‚ï¼Œæ¸…é™¤ç¯„ä¾‹åœ–ç‰‡é¸æ“‡
if page_mode == "ğŸ“¤ ä¸Šå‚³åœ–ç‰‡" and 'selected_example_image' in st.session_state:
    del st.session_state['selected_example_image']

st.markdown("---")

# ä¸Šå‚³åœ–ç‰‡æ¨¡å¼
if page_mode == "ğŸ“¤ ä¸Šå‚³åœ–ç‰‡":
    st.header("ä¸Šå‚³ä½ çš„åœ–ç‰‡")
    uploaded_file = st.file_uploader(
        "é¸æ“‡ä¸€å¼µåœ–ç‰‡...",
        type=['png', 'jpg', 'jpeg', 'bmp'],
        help="æ”¯æ´ PNG, JPG, JPEG, BMP æ ¼å¼"
    )

    if uploaded_file is not None:
        image = Image.open(uploaded_file)

        st.markdown("---")

        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("ğŸš€ é–‹å§‹è¾¨è­˜", use_container_width=True, type="primary"):
                with st.spinner("è¾¨è­˜ä¸­..."):
                    results = predict_image(image, model, confidence, img_size)
                    if results:
                        st.markdown("---")
                        display_results(results, image)

# ä¸Šå‚³å½±ç‰‡æ¨¡å¼
elif page_mode == "ğŸ¬ ä¸Šå‚³å½±ç‰‡":
    st.header("ä¸Šå‚³ä½ çš„å½±ç‰‡")

    uploaded_video = st.file_uploader(
        "é¸æ“‡ä¸€å€‹å½±ç‰‡æª”æ¡ˆ...",
        type=['mp4', 'avi', 'mov', 'mkv'],
        help="æ”¯æ´ MP4, AVI, MOV, MKV æ ¼å¼"
    )

    if uploaded_video is not None:
        # å„²å­˜ä¸Šå‚³çš„å½±ç‰‡åˆ°è‡¨æ™‚æª”æ¡ˆ
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
            tmp_file.write(uploaded_video.read())
            video_path = tmp_file.name

        st.markdown("---")

        # é¡¯ç¤ºåŸå§‹å½±ç‰‡é è¦½
        with st.expander("ğŸ“¹ åŸå§‹å½±ç‰‡é è¦½", expanded=False):
            st.video(video_path)

        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("ğŸš€ é–‹å§‹è¾¨è­˜å½±ç‰‡", use_container_width=True, type="primary"):
                with st.spinner("è™•ç†å½±ç‰‡ä¸­ï¼Œè«‹ç¨å€™..."):
                    output_video_path, all_detections, fps, total_frames = predict_video(
                        video_path, model, confidence, img_size
                    )

                    if output_video_path:
                        st.markdown("---")
                        st.success(f"### âœ… å½±ç‰‡è™•ç†å®Œæˆï¼")
                        st.info(f"å½±ç‰‡è³‡è¨Š: ç¸½å¹€æ•¸ {total_frames}ï¼ŒFPS {fps}")

                        # å˜—è©¦ä½¿ç”¨ ffmpeg é‡æ–°ç·¨ç¢¼ä»¥ç¢ºä¿ç€è¦½å™¨ç›¸å®¹æ€§
                        try:
                            import subprocess

                            # ä½¿ç”¨ ffmpeg é‡æ–°ç·¨ç¢¼ç‚º H.264
                            re_encoded_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name

                            # æª¢æŸ¥æ˜¯å¦æœ‰ ffmpeg
                            result = subprocess.run(
                                ['ffmpeg', '-version'],
                                capture_output=True,
                                timeout=5
                            )

                            if result.returncode == 0:
                                # ä½¿ç”¨ ffmpeg é‡æ–°ç·¨ç¢¼
                                subprocess.run([
                                    'ffmpeg', '-y', '-i', output_video_path,
                                    '-c:v', 'libx264',
                                    '-preset', 'fast',
                                    '-crf', '22',
                                    '-c:a', 'copy',
                                    re_encoded_path
                                ], capture_output=True, check=True)

                                # æ›¿æ›ç‚ºé‡æ–°ç·¨ç¢¼çš„å½±ç‰‡
                                os.unlink(output_video_path)
                                output_video_path = re_encoded_path
                                st.sidebar.info("âœ… ä½¿ç”¨ ffmpeg é‡æ–°ç·¨ç¢¼")
                            else:
                                st.sidebar.warning("âš ï¸ ffmpeg ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹ç·¨ç¢¼")
                        except:
                            # å¦‚æœ ffmpeg å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å½±ç‰‡
                            st.sidebar.warning("âš ï¸ ffmpeg é‡æ–°ç·¨ç¢¼å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹ç·¨ç¢¼")
                            pass

                        # è®€å–å½±ç‰‡æª”æ¡ˆ
                        with open(output_video_path, 'rb') as video_file:
                            video_bytes = video_file.read()

                        # é¡¯ç¤ºè™•ç†å¾Œçš„å½±ç‰‡
                        st.subheader("ğŸ¬ æª¢æ¸¬çµæœå½±ç‰‡")
                        st.video(video_bytes)

                        # æä¾›ä¸‹è¼‰æŒ‰éˆ•
                        st.download_button(
                            label="â¬‡ï¸ ä¸‹è¼‰è™•ç†å¾Œçš„å½±ç‰‡",
                            data=video_bytes,
                            file_name="detected_video.mp4",
                            mime="video/mp4"
                        )

                        # çµ±è¨ˆåˆ†æ
                        if all_detections:
                            st.markdown("---")
                            st.subheader("ğŸ“ˆ çµ±è¨ˆåˆ†æ")

                            from collections import Counter
                            detection_counts = Counter(all_detections)
                            total_objects = len(all_detections)

                            col1, col2 = st.columns(2)

                            with col1:
                                st.write("**æª¢æ¸¬çµæœçµ±è¨ˆ:**")
                                for class_name, count in detection_counts.most_common():
                                    st.write(f"- {class_name}: {count} æ¬¡ ({count/total_objects*100:.1f}%)")

                            with col2:
                                st.write("**æœ€å¸¸æª¢æ¸¬åˆ°çš„èŠ±è‰²:**")
                                most_common = detection_counts.most_common(1)[0]
                                st.metric("èŠ±è‰²", most_common[0], f"{most_common[1]} æ¬¡")
                                st.metric("ç¸½æª¢æ¸¬æ•¸", total_objects)
                                st.metric("å¹³å‡æ¯å¹€æª¢æ¸¬æ•¸", f"{total_objects/total_frames:.2f}")

                        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                        try:
                            os.unlink(output_video_path)
                        except:
                            pass

                # æ¸…ç†è¼¸å…¥å½±ç‰‡è‡¨æ™‚æª”æ¡ˆ
                try:
                    os.unlink(video_path)
                except:
                    pass

# é¸æ“‡ç¯„ä¾‹åœ–ç‰‡æ¨¡å¼
elif page_mode == "ğŸ“ é¸æ“‡ç¯„ä¾‹åœ–ç‰‡":
    st.header("å¾è³‡æ–™é›†ä¸­é¸æ“‡ç¯„ä¾‹åœ–ç‰‡")

    # ç²å– data è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰åœ–ç‰‡
    data_path = Path(__file__).parent / "data"

    if data_path.exists():
        image_files = sorted(list(data_path.glob("*.png")) + list(data_path.glob("*.jpg")))

        if len(image_files) > 0:
            # æª¢æŸ¥æ˜¯å¦å·²é¸æ“‡åœ–ç‰‡
            if 'selected_example_image' not in st.session_state:
                # é¡¯ç¤ºåœ–ç‰‡ç¸®åœ–ä¾›é¸æ“‡
                st.write(f"å…±æœ‰ {len(image_files)} å¼µç¯„ä¾‹åœ–ç‰‡ï¼Œé»æ“Šã€Œé¸æ“‡ã€å³å¯è‡ªå‹•è¾¨è­˜")

                # ä½¿ç”¨ç¶²æ ¼ä½ˆå±€é¡¯ç¤ºåœ–ç‰‡
                cols = st.columns(5)

                for idx, img_path in enumerate(image_files):
                    col = cols[idx % 5]
                    with col:
                        img = Image.open(img_path)
                        st.image(img, caption=img_path.name, use_container_width=True)
                        if st.button(f"é¸æ“‡", key=f"select_{idx}"):
                            # é¸æ“‡åœ–ç‰‡ä¸¦è‡ªå‹•é€²è¡Œæ¨è«–
                            st.session_state['selected_example_image'] = img_path
                            st.rerun()
            else:
                # å·²é¸æ“‡åœ–ç‰‡ï¼Œé¡¯ç¤ºçµæœ
                selected_path = st.session_state['selected_example_image']

                # é¡¯ç¤ºç¸®åœ–ç¶²æ ¼ï¼ˆå¯æ”¶åˆï¼‰
                with st.expander("ğŸ“‚ ç€è¦½å…¶ä»–ç¯„ä¾‹åœ–ç‰‡", expanded=False):
                    st.write(f"å…±æœ‰ {len(image_files)} å¼µç¯„ä¾‹åœ–ç‰‡")
                    cols = st.columns(5)

                    for idx, img_path in enumerate(image_files):
                        col = cols[idx % 5]
                        with col:
                            img = Image.open(img_path)
                            st.image(img, caption=img_path.name, use_container_width=True)
                            if st.button(f"é¸æ“‡", key=f"select_exp_{idx}"):
                                st.session_state['selected_example_image'] = img_path
                                st.rerun()

                # é¡¯ç¤ºè¾¨è­˜çµæœæ¨™é¡Œ
                st.success(f"### âœ… å·²é¸æ“‡ä¸¦è¾¨è­˜: {selected_path.name}")

                # é‡æ–°é¸æ“‡æŒ‰éˆ•
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    if st.button("ğŸ”„ é‡æ–°é¸æ“‡å…¶ä»–åœ–ç‰‡", use_container_width=True):
                        del st.session_state['selected_example_image']
                        st.rerun()

                st.markdown("---")

                # è¼‰å…¥åœ–ç‰‡ä¸¦é€²è¡Œæ¨è«–
                image = Image.open(selected_path)

                with st.spinner("è¾¨è­˜ä¸­..."):
                    results = predict_image(image, model, confidence, img_size)
                    if results:
                        display_results(results, image)
        else:
            st.warning("data è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡")
    else:
        st.error("æ‰¾ä¸åˆ° data è³‡æ–™å¤¾")

# é å°¾
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center'>
        <p>ğŸ’¡ æç¤º: èª¿æ•´å´é‚Šæ¬„çš„åƒæ•¸ä»¥æ”¹è®Šæ¨è«–è¨­å®š</p>
    </div>
    """,
    unsafe_allow_html=True
)
